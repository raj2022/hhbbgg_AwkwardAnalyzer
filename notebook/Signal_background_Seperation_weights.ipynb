{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553e5fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import uproot\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, losses\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Define file path and tree names\n",
    "files = [\n",
    "    (\"../outputfiles/hhbbgg_analyzer-trees.root\", \"/GluGluToHH/srbbgg\"),\n",
    "    (\"../outputfiles/hhbbgg_analyzer-trees.root\", \"/GGJets/srbbgg\"),\n",
    "    (\"../outputfiles/hhbbgg_analyzer-trees.root\", \"/GJetPt20To40/srbbgg\"),\n",
    "    (\"../outputfiles/hhbbgg_analyzer-trees.root\", \"/GJetPt40/srbbgg\")\n",
    "]\n",
    "keys = [\n",
    "    'dibjet_mass',\n",
    "    'diphoton_mass',\n",
    "    'bbgg_mass',\n",
    "    'dibjet_pt',\n",
    "    'diphoton_pt',\n",
    "    'bbgg_pt',\n",
    "    'lead_pho_pt',\n",
    "    'sublead_pho_pt',\n",
    "    'bbgg_eta',\n",
    "    'bbgg_phi',\n",
    "    'lead_pho_eta',\n",
    "    'lead_pho_phi',\n",
    "    'sublead_pho_eta',\n",
    "    'sublead_pho_phi',\n",
    "    'diphoton_eta',\n",
    "    'diphoton_phi',\n",
    "    'dibjet_eta',\n",
    "    'dibjet_phi',\n",
    "    'lead_bjet_pt',\n",
    "    'sublead_bjet_pt',\n",
    "    'lead_bjet_eta',\n",
    "    'lead_bjet_phi',\n",
    "    'sublead_bjet_eta',\n",
    "    'sublead_bjet_phi',\n",
    "    'sublead_bjet_PNetB',\n",
    "    'lead_bjet_PNetB',\n",
    "    'CosThetaStar_gg',\n",
    "    'CosThetaStar_jj',\n",
    "    'CosThetaStar_CS',\n",
    "    'DeltaR_jg_min',\n",
    "    'pholead_PtOverM',\n",
    "    'phosublead_PtOverM',\n",
    "    'FirstJet_PtOverM',\n",
    "    'SecondJet_PtOverM',\n",
    "    'lead_pt_over_diphoton_mass',\n",
    "    'sublead_pt_over_diphoton_mass',\n",
    "    'lead_pt_over_dibjet_mass',\n",
    "    'sublead_pt_over_dibjet_mass',\n",
    "    'diphoton_bbgg_mass',\n",
    "    'dibjet_bbgg_mass',\n",
    "    'weight_preselection',\n",
    "]\n",
    "\n",
    "dfs = {}\n",
    "\n",
    "# Loop through each file and load the corresponding dataframe\n",
    "for file, key in files:\n",
    "    with uproot.open(file) as f:\n",
    "        dfs[key] = f[key].arrays(keys, library=\"pd\")\n",
    "\n",
    "signal_df = dfs[\"/GluGluToHH/srbbgg\"]\n",
    "background_df_1 = dfs[\"/GGJets/srbbgg\"]\n",
    "background_df_2 = dfs[\"/GJetPt20To40/srbbgg\"]\n",
    "background_df_3 = dfs[\"/GJetPt40/srbbgg\"]\n",
    "\n",
    "print('singal df', signal_df.shape)\n",
    "print('background_df_1 ', background_df_1.shape)\n",
    "print('background_df_2', background_df_2.shape)\n",
    "print('background_df_1 ', background_df_3.shape)\n",
    "\n",
    "background_df = pd.concat([background_df_1, background_df_2, background_df_3], ignore_index=True)\n",
    "print('background_df', background_df.shape)\n",
    "\n",
    "signal_df['label'] = 1\n",
    "background_df['label'] = 0\n",
    "\n",
    "combined_df = pd.concat([signal_df, background_df], ignore_index=True)\n",
    "\n",
    "features = [\n",
    "    'diphoton_mass',\n",
    "    'dibjet_mass',\n",
    "    'lead_pho_pt',\n",
    "    'sublead_pho_pt',\n",
    "    'bbgg_eta',\n",
    "    'bbgg_phi',\n",
    "    'bbgg_mass',\n",
    "    'lead_pho_eta',\n",
    "    'lead_pho_phi',\n",
    "    'sublead_pho_eta',\n",
    "    'sublead_pho_phi',\n",
    "    'diphoton_eta',\n",
    "    'diphoton_phi',\n",
    "    'dibjet_eta',\n",
    "    'dibjet_phi',\n",
    "    'lead_bjet_pt',\n",
    "    'sublead_bjet_pt',\n",
    "    'lead_bjet_eta',\n",
    "    'lead_bjet_phi',\n",
    "    'sublead_bjet_eta',\n",
    "    'sublead_bjet_phi',\n",
    "    'sublead_bjet_PNetB',\n",
    "    'lead_bjet_PNetB',\n",
    "    'CosThetaStar_gg',\n",
    "    'CosThetaStar_jj',\n",
    "    'CosThetaStar_CS',\n",
    "    'DeltaR_jg_min',\n",
    "    'pholead_PtOverM',\n",
    "    'phosublead_PtOverM',\n",
    "    'FirstJet_PtOverM',\n",
    "    'SecondJet_PtOverM',\n",
    "    'lead_pt_over_diphoton_mass',\n",
    "    'sublead_pt_over_diphoton_mass',\n",
    "    'lead_pt_over_dibjet_mass',\n",
    "    'sublead_pt_over_dibjet_mass',\n",
    "    'diphoton_bbgg_mass',\n",
    "    'dibjet_bbgg_mass',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6db99f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = combined_df[features]\n",
    "y = combined_df['label']\n",
    "weight = combined_df['weight_preselection']\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_df[features], combined_df['label'], test_size=0.2, random_state=42, stratify=combined_df['label'])\n",
    "\n",
    "# Extract the weights for train and test datasets\n",
    "X_train_weights = combined_df.loc[X_train.index, 'weight_preselection']\n",
    "X_test_weights = combined_df.loc[X_test.index, 'weight_preselection']\n",
    "\n",
    "# Impute and scale the features\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "X_train_weights_tensor = torch.tensor(X_train_weights.values, dtype=torch.float32)\n",
    "X_test_weights_tensor = torch.tensor(X_test_weights.values, dtype=torch.float32)\n",
    "\n",
    "# Create TensorDataset and DataLoader\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor, X_train_weights_tensor)\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor, X_test_weights_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae12da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dnn_model(input_dim):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(256, input_dim=input_dim, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39018ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(input_dim, X_train, y_train, sample_weights):\n",
    "    model = create_dnn_model(input_dim)\n",
    "\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "                  loss=losses.BinaryCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    os.makedirs(\"bdtplots/dnn\", exist_ok=True)\n",
    "    with open(\"bdtplots/dnn/model_summary.txt\", \"w\") as f:\n",
    "        model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    model_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        sample_weight=sample_weights,\n",
    "                        epochs=50,\n",
    "                        batch_size=32,\n",
    "                        validation_split=0.2,\n",
    "                        callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "    return model, history\n",
    "\n",
    "input_dim = len(features)\n",
    "model_path = 'best_model.keras'\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    model = create_dnn_model(input_dim)\n",
    "    model.load_weights(model_path)\n",
    "    print(\"Model loaded from disk.\")\n",
    "else:\n",
    "    model, history = train_model(input_dim, X_train, y_train, X_train_weights)\n",
    "    model.save_weights(model_path)\n",
    "    print(\"Model trained and saved to disk.\")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c6b510",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "accuracy = history_dict['accuracy']\n",
    "val_accuracy = history_dict['val_accuracy']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"bdtplots/dnn/srbbgg_training_history.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ec172c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the best model for evaluation\n",
    "from tensorflow.keras.models import load_model\n",
    "# model = load_model('best_model.keras')\n",
    "\n",
    "# Model prediction and evaluation\n",
    "y_train_pred = model.predict(X_train).squeeze()\n",
    "y_test_pred = model.predict(X_test).squeeze()\n",
    "\n",
    "y_train_pred_class = (y_train_pred > 0.5).astype(int)\n",
    "y_test_pred_class = (y_test_pred > 0.5).astype(int)\n",
    "\n",
    "# Custom weighted accuracy\n",
    "def weighted_accuracy(y_true, y_pred, weights):\n",
    "    return np.sum(weights * (y_true == y_pred)) / np.sum(weights)\n",
    "\n",
    "train_accuracy = weighted_accuracy(y_train, y_train_pred_class, X_train_weights)\n",
    "test_accuracy = weighted_accuracy(y_test, y_test_pred_class, X_test_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfb56e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom weighted ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_test_pred, sample_weight=X_test_weights)\n",
    "\n",
    "print(\"Weighted accuracy on test set:\", test_accuracy)\n",
    "print(\"Weighted ROC AUC on test set:\", roc_auc)\n",
    "print(classification_report(y_test, y_test_pred_class))\n",
    "\n",
    "# Optionally, plot weighted ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_pred, sample_weight=X_test_weights)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"bdtplots/dnn/srbbgg_roc-curve.png\")\n",
    "plt.savefig(\"bdtplots/dnn/srbbgg_roc-curve.pdf\")\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5478b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training accuracy over epochs\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(history.history['accuracy']) + 1), history.history['accuracy'], label='Training Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training Accuracy Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"bdtplots/dnn/srbbgg_training_accuracy.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61710895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "# Load the best model\n",
    "# model = load_model('best_model.keras')\n",
    "\n",
    "# Model prediction and evaluation\n",
    "y_train_pred = model.predict(X_train).squeeze()\n",
    "y_test_pred = model.predict(X_test).squeeze()\n",
    "\n",
    "y_train_pred_class = (y_train_pred > 0.5).astype(int)\n",
    "y_test_pred_class = (y_test_pred > 0.5).astype(int)\n",
    "\n",
    "# Calculate histograms and bins for training data with weights\n",
    "train_hist_s, bins = np.histogram(y_train_pred[y_train == 1], bins=30, density=True, weights=X_train_weights[y_train == 1])\n",
    "train_hist_r, _ = np.histogram(y_train_pred[y_train == 0], bins=bins, density=True, weights=X_train_weights[y_train == 0])\n",
    "\n",
    "# Calculate bin centers\n",
    "bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "# Plot histograms for training data\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "plt.hist(y_train_pred[y_train == 1], bins=30, color='blue', alpha=0.5, label='S (Train)', density=True, weights=X_train_weights[y_train == 1])\n",
    "plt.hist(y_train_pred[y_train == 0], bins=30, color='red', alpha=0.5, label='R (Train)', density=True, weights=X_train_weights[y_train == 0])\n",
    "\n",
    "# Plot scatter points for test data over the top of training histograms with weights\n",
    "plt.scatter(bin_centers, np.histogram(y_test_pred[y_test == 1], bins=bins, density=True, weights=X_test_weights[y_test == 1])[0],\n",
    "            color='blue', alpha=0.5, label='S (Test)', marker='o', s=30, edgecolor='k')\n",
    "plt.scatter(bin_centers, np.histogram(y_test_pred[y_test == 0], bins=bins, density=True, weights=X_test_weights[y_test == 0])[0],\n",
    "            color='red', alpha=0.5, label='B (Test)', marker='o', s=30, edgecolor='k')\n",
    "\n",
    "plt.axvline(0.5, color='k', linestyle='--')\n",
    "plt.xlabel('Classifier output')\n",
    "plt.ylabel('Normalized Yields')\n",
    "plt.legend()\n",
    "plt.title('Classification with Keras')\n",
    "\n",
    "# Save and display the plot\n",
    "# plt.savefig(\"bdtplots/dnn/classifier_output_plot.png\")\n",
    "# plt.savefig(\"bdtplots/dnn/classifier_output_plot.pdf\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877c4dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import uproot\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torchsummary import summary\n",
    "from tensorflow.keras import layers, models, optimizers, losses, regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define file path and tree names\n",
    "files = [\n",
    "    (\"../outputfiles/hhbbgg_analyzer-trees.root\", \"/GluGluToHH/preselection\"),\n",
    "    (\"../outputfiles/hhbbgg_analyzer-trees.root\", \"/GGJets/preselection\"),\n",
    "    (\"../outputfiles/hhbbgg_analyzer-trees.root\", \"/GJetPt20To40/preselection\"),\n",
    "    (\"../outputfiles/hhbbgg_analyzer-trees.root\", \"/GJetPt40/preselection\")\n",
    "]\n",
    "keys = [\n",
    "    'dibjet_mass',\n",
    "    'diphoton_mass',\n",
    "    'bbgg_mass',\n",
    "    'dibjet_pt',\n",
    "    'diphoton_pt',\n",
    "    'bbgg_pt',\n",
    "    'lead_pho_pt',\n",
    "    'sublead_pho_pt',\n",
    "    'bbgg_eta',\n",
    "    'bbgg_phi',\n",
    "    'lead_pho_eta',\n",
    "    'lead_pho_phi',\n",
    "    'sublead_pho_eta',\n",
    "    'sublead_pho_phi',\n",
    "    'diphoton_eta',\n",
    "    'diphoton_phi',\n",
    "    'dibjet_eta',\n",
    "    'dibjet_phi',\n",
    "    'lead_bjet_pt',\n",
    "    'sublead_bjet_pt',\n",
    "    'lead_bjet_eta',\n",
    "    'lead_bjet_phi',\n",
    "    'sublead_bjet_eta',\n",
    "    'sublead_bjet_phi',\n",
    "    'sublead_bjet_PNetB',\n",
    "    'lead_bjet_PNetB',\n",
    "    'CosThetaStar_gg',\n",
    "    'CosThetaStar_jj',\n",
    "    'CosThetaStar_CS',\n",
    "    'DeltaR_jg_min',\n",
    "    'pholead_PtOverM',\n",
    "    'phosublead_PtOverM',\n",
    "    'FirstJet_PtOverM',\n",
    "    'SecondJet_PtOverM',\n",
    "    'lead_pt_over_diphoton_mass',\n",
    "    'sublead_pt_over_diphoton_mass',\n",
    "    'lead_pt_over_dibjet_mass',\n",
    "    'sublead_pt_over_dibjet_mass',\n",
    "    'diphoton_bbgg_mass',\n",
    "    'dibjet_bbgg_mass',\n",
    "    'weight_preselection',\n",
    "]\n",
    "\n",
    "\n",
    "# Initialize an empty dictionary to store dataframes\n",
    "dfs = {}\n",
    "\n",
    "# Loop through each file and load the corresponding dataframe\n",
    "for file, key in files:\n",
    "    with uproot.open(file) as f:\n",
    "        dfs[key] = f[key].arrays(keys, library=\"pd\")\n",
    "\n",
    "# Access your dataframes by key\n",
    "signal_df = dfs[\"/GluGluToHH/preselection\"]\n",
    "background_df_1 = dfs[\"/GGJets/preselection\"]\n",
    "background_df_2 = dfs[\"/GJetPt20To40/preselection\"]\n",
    "background_df_3 = dfs[\"/GJetPt40/preselection\"]\n",
    "\n",
    "\n",
    "weight = 'weight_preselection'\n",
    "\n",
    "print('singal df', signal_df.shape)\n",
    "print('background_df_1 ', background_df_1.shape)\n",
    "print('background_df_2', background_df_2.shape)\n",
    "print('background_df_1 ', background_df_3.shape)\n",
    "\n",
    "\n",
    "background_df = pd.concat([background_df_1, background_df_2, background_df_3], ignore_index=True)\n",
    "# background_df = background_df_1\n",
    "print('background_df', background_df.shape)\n",
    "\n",
    "signal_df['label'] = 1\n",
    "background_df['label'] = 0\n",
    "\n",
    "combined_df = pd.concat([signal_df, background_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c452007",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce31cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6895d7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'diphoton_mass',\n",
    "    'dibjet_mass',\n",
    "    'lead_pho_pt',\n",
    "    'sublead_pho_pt',\n",
    "    'bbgg_eta',\n",
    "    'bbgg_phi',\n",
    "    'lead_pho_eta',\n",
    "    'lead_pho_phi',\n",
    "    'sublead_pho_eta',\n",
    "    'sublead_pho_phi',\n",
    "    'diphoton_eta',\n",
    "    'diphoton_phi',\n",
    "    'dibjet_eta',\n",
    "    'dibjet_phi',\n",
    "    'lead_bjet_pt',\n",
    "    'sublead_bjet_pt',\n",
    "    'lead_bjet_eta',\n",
    "    'lead_bjet_phi',\n",
    "    'sublead_bjet_eta',\n",
    "    'sublead_bjet_phi',\n",
    "    'sublead_bjet_PNetB',\n",
    "    'lead_bjet_PNetB',\n",
    "    'CosThetaStar_gg',\n",
    "    'CosThetaStar_jj',\n",
    "    'CosThetaStar_CS',\n",
    "    'DeltaR_jg_min',\n",
    "    'pholead_PtOverM',\n",
    "    'phosublead_PtOverM',\n",
    "    'FirstJet_PtOverM',\n",
    "    'SecondJet_PtOverM',\n",
    "    'lead_pt_over_diphoton_mass',\n",
    "    'sublead_pt_over_diphoton_mass',\n",
    "    'lead_pt_over_dibjet_mass',\n",
    "    'sublead_pt_over_dibjet_mass',\n",
    "    'diphoton_bbgg_mass',\n",
    "    'dibjet_bbgg_mass',\n",
    "]\n",
    "\n",
    "X = combined_df[features]\n",
    "y = combined_df['label']\n",
    "weight = combined_df['weight_preselection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af370ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8924e70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_df[features], combined_df['label'], test_size=0.2, random_state=42, stratify=combined_df['label'])\n",
    "\n",
    "# Extract the weights for train and test datasets\n",
    "X_train_weights = combined_df.loc[X_train.index, 'weight_preselection']\n",
    "X_test_weights = combined_df.loc[X_test.index, 'weight_preselection']\n",
    "\n",
    "# Impute and scale the features\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "X_train_weights_tensor = torch.tensor(X_train_weights.values, dtype=torch.float32)\n",
    "X_test_weights_tensor = torch.tensor(X_test_weights.values, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f4c238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorDataset and DataLoader\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor, X_train_weights_tensor)\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor, X_test_weights_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7d3c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, losses\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Define the model\n",
    "def create_dnn_model(input_dim):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(256, input_dim=input_dim, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "input_dim = len(features)  # Assuming 'features' is defined\n",
    "model = create_dnn_model(input_dim)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "              loss=losses.BinaryCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Save the model summary to a file\n",
    "os.makedirs(\"bdtplots/dnn\", exist_ok=True)\n",
    "with open(\"bdtplots/dnn/model_summary.txt\", \"w\") as f:\n",
    "    model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "\n",
    "# Define early stopping and model checkpoint\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=50,\n",
    "                    batch_size=16,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "# Load the best model\n",
    "model.load_weights('best_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c69324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract training history\n",
    "history_dict = history.history\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "accuracy = history_dict['accuracy']\n",
    "val_accuracy = history_dict['val_accuracy']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8699019e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Assuming you have defined X_train, X_test, y_train, and y_test appropriately\n",
    "# Load the best model\n",
    "model = load_model('best_model.h5')\n",
    "\n",
    "# Model prediction and evaluation\n",
    "y_train_pred = model.predict(X_train).squeeze()\n",
    "y_test_pred = model.predict(X_test).squeeze()\n",
    "\n",
    "y_train_pred_class = (y_train_pred > 0.5).astype(int)\n",
    "y_test_pred_class = (y_test_pred > 0.5).astype(int)\n",
    "\n",
    "# Calculate histograms and bins for training data\n",
    "train_hist_s, bins = np.histogram(y_train_pred[y_train == 1], bins=30, density=True)\n",
    "train_hist_r, _ = np.histogram(y_train_pred[y_train == 0], bins=bins, density=True)\n",
    "\n",
    "# Calculate bin centers\n",
    "bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "# Plot histograms for training data\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "plt.hist(y_train_pred[y_train == 1], bins=30, color='blue', alpha=0.5, label='S (Train)', density=True)\n",
    "plt.hist(y_train_pred[y_train == 0], bins=30, color='red', alpha=0.5, label='R (Train)', density=True)\n",
    "\n",
    "# Plot scatter points for test data over the top of training histograms\n",
    "plt.scatter(bin_centers, np.histogram(y_test_pred[y_test == 1], bins=bins, density=True)[0],\n",
    "            color='blue', alpha=0.5, label='S (Test)', marker='o', s=30, edgecolor='k')\n",
    "plt.scatter(bin_centers, np.histogram(y_test_pred[y_test == 0], bins=bins, density=True)[0],\n",
    "            color='red', alpha=0.5, label='B (Test)', marker='o', s=30, edgecolor='k')\n",
    "\n",
    "plt.axvline(0.5, color='k', linestyle='--')\n",
    "plt.xlabel('Classifier output')\n",
    "plt.ylabel('Normalized Yields')\n",
    "plt.legend()\n",
    "plt.title('Classification with scikit-learn')\n",
    "\n",
    "# Save and display the plot\n",
    "# plt.savefig(\"bdtplots/dnn/classifier_output_plot.png\")\n",
    "# plt.savefig(\"bdtplots/dnn/classifier_output_plot.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fa7316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Assuming you have defined X_train, X_test, y_train, and y_test appropriately\n",
    "# Load the best model\n",
    "model = load_model('best_model.h5')\n",
    "\n",
    "# Model prediction and evaluation\n",
    "y_train_pred = model.predict(X_train).squeeze()\n",
    "y_test_pred = model.predict(X_test).squeeze()\n",
    "\n",
    "y_train_pred_class = (y_train_pred > 0.5).astype(int)\n",
    "y_test_pred_class = (y_test_pred > 0.5).astype(int)\n",
    "\n",
    "# Calculate histograms and bins for training data\n",
    "train_hist_s, bins = np.histogram(y_train_pred[y_train == 1], bins=30, density=True)\n",
    "train_hist_r, _ = np.histogram(y_train_pred[y_train == 0], bins=bins, density=True)\n",
    "\n",
    "# Calculate bin centers\n",
    "bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "# Plot histograms for training data\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Add background colors\n",
    "plt.axvspan(0, 0.5, color='red', alpha=0.1)\n",
    "plt.axvspan(0.5, 1, color='blue', alpha=0.1)\n",
    "\n",
    "plt.hist(y_train_pred[y_train == 1], bins=30, color='blue', alpha=0.5, label='S (Train)', density=True)\n",
    "plt.hist(y_train_pred[y_train == 0], bins=30, color='red', alpha=0.5, label='B (Train)', density=True)\n",
    "\n",
    "# Plot scatter points for test data over the top of training histograms\n",
    "plt.scatter(bin_centers, np.histogram(y_test_pred[y_test == 1], bins=bins, density=True)[0],\n",
    "            color='blue', alpha=0.5, label='S (Test)', marker='o', s=50, edgecolor='k')\n",
    "plt.scatter(bin_centers, np.histogram(y_test_pred[y_test == 0], bins=bins, density=True)[0],\n",
    "            color='red', alpha=0.5, label='B (Test)', marker='o', s=50, edgecolor='k')\n",
    "\n",
    "plt.axvline(0.5, color='k', linestyle='--')\n",
    "plt.xlim(0,1)\n",
    "plt.xlabel('Classifier output')\n",
    "plt.ylabel('Normalized Yields')\n",
    "plt.legend()\n",
    "plt.title('Classification with scikit-learn')\n",
    "\n",
    "# Save and display the plot\n",
    "# plt.savefig(\"bdtplots/dnn/classifier_output_plot.png\")\n",
    "# plt.savefig(\"bdtplots/dnn/classifier_output_plot.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acce5387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "conf_matrix = pd.crosstab(y_test, y_test_pred_class, rownames=['Actual'], colnames=['Predicted'])\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "# plt.savefig(\"bdtplots/dnn/confusion_matrix.png\")\n",
    "# plt.savefig(\"bdtplots/dnn/confusion_matrix.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61f0396",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "# plt.savefig(\"bdtplots/dnn/roc_curve.png\")\n",
    "# plt.savefig(\"bdtplots/dnn/roc_curve.pdf\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "print(\"Accuracy on test set:\", accuracy_score(y_test, y_test_pred_class))\n",
    "print(\"ROC AUC on test set:\", roc_auc_score(y_test, y_test_pred))\n",
    "print(classification_report(y_test, y_test_pred_class))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7817c45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming `history` is the history object returned by `model.fit`\n",
    "# Plot training accuracy over epochs\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(history.history['accuracy']) + 1), history.history['accuracy'], label='Training Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training Accuracy Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"bdtplots/dnn/training_accuracy.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24894d95",
   "metadata": {},
   "source": [
    "### Model improvement: I\n",
    "This following model will be overtrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d3ac18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_df[features], combined_df['label'], test_size=0.2, random_state=42, stratify=combined_df['label'], shuffle=True)\n",
    "\n",
    "# Extract the weights for train and test datasets\n",
    "X_train_weights = combined_df.loc[X_train.index, 'weight_preselection']\n",
    "X_test_weights = combined_df.loc[X_test.index, 'weight_preselection']\n",
    "\n",
    "# Impute and scale the features\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Define the model\n",
    "def create_dnn_model(input_dim):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(256, input_dim=input_dim, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "model = create_dnn_model(input_dim)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "              loss=losses.BinaryCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Save the model summary to a file\n",
    "import os\n",
    "os.makedirs(\"bdtplots/dnn\", exist_ok=True)\n",
    "with open(\"bdtplots/dnn/model_summary.txt\", \"w\") as f:\n",
    "    model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train,\n",
    "                    sample_weight=X_train_weights,\n",
    "                    epochs=50,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(X_test_scaled, y_test, X_test_weights),\n",
    "                    callbacks=[early_stopping, model_checkpoint, reduce_lr])\n",
    "\n",
    "# Load the best model\n",
    "model.load_weights('best_model.h5')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract training history\n",
    "history_dict = history.history\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "accuracy = history_dict['accuracy']\n",
    "val_accuracy = history_dict['val_accuracy']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

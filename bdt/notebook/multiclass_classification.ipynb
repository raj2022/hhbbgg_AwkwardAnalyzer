{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import uproot\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import Adam\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# File paths\n",
    "signal_files_lowX_lowY = [\n",
    "    (\"../../outputfiles/hhbbgg_analyzer_NMSSMv2-trees.root\", \"/NMSSM_X300_Y60/preselection\"),\n",
    "    (\"../../outputfiles/hhbbgg_analyzer_NMSSMv2-trees.root\", \"/NMSSM_X300_Y70/preselection\"),\n",
    "    (\"../../outputfiles/hhbbgg_analyzer_NMSSMv2-trees.root\", \"/NMSSM_X300_Y80/preselection\"),\n",
    "    (\"../../outputfiles/hhbbgg_analyzer_NMSSMv2-trees.root\", \"/NMSSM_X300_Y90/preselection\"),\n",
    "    (\"../../outputfiles/hhbbgg_analyzer_NMSSMv2-trees.root\", \"/NMSSM_X400_Y60/preselection\"),\n",
    "    (\"../../outputfiles/hhbbgg_analyzer_NMSSMv2-trees.root\", \"/NMSSM_X400_Y70/preselection\"),\n",
    "    (\"../../outputfiles/hhbbgg_analyzer_NMSSMv2-trees.root\", \"/NMSSM_X400_Y80/preselection\"),\n",
    "    (\"../../outputfiles/hhbbgg_analyzer_NMSSMv2-trees.root\", \"/NMSSM_X400_Y90/preselection\"),\n",
    "#     (\"../../outputfiles/hhbbgg_analyzerNMSSM-trees.root\", \"/NMSSM_X550_Y80/preselection\"),\n",
    "#     (\"../../outputfiles/hhbbgg_analyzerNMSSM-trees.root\", \"/NMSSM_X550_Y90/preselection\"),\n",
    "# #     (\"../../outputfiles/hhbbgg_analyzerNMSSM-trees.root\", \"/NMSSM_X550_Y95/preselection\"),\n",
    "# #     (\"../../outputfiles/hhbbgg_analyzerNMSSM-trees.root\", \"/NMSSM_X550_Y100/preselection\"),\n",
    "]\n",
    "\n",
    "background_files = [\n",
    "    (\"../../outputfiles/hhbbgg_analyzer_v2-trees.root\", \"/GGJets/preselection\"),\n",
    "    (\"../../outputfiles/hhbbgg_analyzer_v2-trees.root\", \"/GJetPt20to40/preselection\"),\n",
    "    (\"../../outputfiles/hhbbgg_analyzer_v2-trees.root\", \"/GJetPt40/preselection\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to be loaded\n",
    "keys = [\n",
    "     'dibjet_pt', 'diphoton_pt', 'bbgg_pt', 'bbgg_eta', 'bbgg_phi',\n",
    "    'lead_pho_phi', 'sublead_pho_eta', 'sublead_pho_phi', 'diphoton_eta', \n",
    "    'diphoton_phi', 'dibjet_eta', 'dibjet_phi', 'lead_bjet_pt', 'sublead_bjet_pt', \n",
    "    'lead_bjet_eta', 'lead_bjet_phi', 'sublead_bjet_eta', 'sublead_bjet_phi', \n",
    "    'sublead_bjet_PNetB', 'lead_bjet_PNetB', 'CosThetaStar_gg', 'CosThetaStar_jj', \n",
    "    'CosThetaStar_CS', 'DeltaR_jg_min',   'pholead_PtOverM', 'phosublead_PtOverM',\n",
    "    'weight_preselection',\n",
    "]\n",
    "# Variables removed :- 'bbgg_mass','FirstJet_PtOverM', 'SecondJet_PtOverM', 'diphoton_bbgg_mass', 'dibjet_bbgg_mass', 'lead_pho_eta',\n",
    "\n",
    "## Add "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Signal Shape: (204445, 27)\n",
      "Total Background Shape: (245708, 27)\n"
     ]
    }
   ],
   "source": [
    "# Load DataFrames\n",
    "dfs = {}\n",
    "\n",
    "# Load signal files\n",
    "for file, key in signal_files_lowX_lowY:\n",
    "    try:\n",
    "        with uproot.open(file) as f:\n",
    "            dfs[key] = f[key].arrays(keys, library=\"pd\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file} with key {key}: {e}\")\n",
    "\n",
    "# Load background files\n",
    "for file, key in background_files:\n",
    "    try:\n",
    "        with uproot.open(file) as f:\n",
    "            dfs[key] = f[key].arrays(keys, library=\"pd\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file} with key {key}: {e}\")\n",
    "\n",
    "# Combine signal DataFrames\n",
    "signal_df = pd.concat([dfs[key] for key in dfs if 'NMSSM' in key], ignore_index=True)\n",
    "background_df = pd.concat([dfs[key] for key in dfs if 'GJet' in key or 'GGJets' in key], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "# signal_df = pd.concat([])\n",
    "# Print combined sample sizes\n",
    "print(f'Total Signal Shape: {signal_df.shape}')\n",
    "print(f'Total Background Shape: {background_df.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign labels to signal and background classes\n",
    "signal_df['label'] = 0  # Signal class\n",
    "\n",
    "# Assign background labels based on the key\n",
    "dfs[\"/GGJets/preselection\"]['label'] = 1  # Background class 1\n",
    "dfs[\"/GJetPt20to40/preselection\"]['label'] = 2  # Background class 2\n",
    "dfs[\"/GJetPt40/preselection\"]['label'] = 2  # Background class 2\n",
    "\n",
    "# Combine signal and all background dataframes into one DataFrame\n",
    "combined_df = pd.concat([signal_df, dfs[\"/GGJets/preselection\"], dfs[\"/GJetPt20to40/preselection\"], dfs[\"/GJetPt40/preselection\"]], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Combine signal and background data\n",
    "# combined_df = pd.concat([signal_df, background_df], ignore_index=True)\n",
    "# print(f'Combined DataFrame Shape: {combined_df.shape}')\n",
    "\n",
    "# Define features and labels\n",
    "features = [\n",
    "    'bbgg_eta', 'bbgg_phi', 'lead_pho_phi', 'sublead_pho_eta', \n",
    "    'sublead_pho_phi', 'diphoton_eta', 'diphoton_phi', 'dibjet_eta', 'dibjet_phi', \n",
    "    'lead_bjet_pt', 'sublead_bjet_pt', 'lead_bjet_eta', 'lead_bjet_phi', 'sublead_bjet_eta', \n",
    "    'sublead_bjet_phi', 'sublead_bjet_PNetB', 'lead_bjet_PNetB', 'CosThetaStar_gg', \n",
    "    'CosThetaStar_jj', 'CosThetaStar_CS', 'DeltaR_jg_min', 'pholead_PtOverM', \n",
    "    'phosublead_PtOverM'\n",
    "]\n",
    "\n",
    "# variables removed:  'bbgg_mass',  'lead_pho_eta','FirstJet_PtOverM', 'SecondJet_PtOverM', 'diphoton_bbgg_mass', 'dibjet_bbgg_mass'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = combined_df[features]\n",
    "y = combined_df['label']\n",
    "weights = combined_df['weight_preselection']\n",
    "\n",
    "# Impute missing values and scale the data\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Extract weights for train and test datasets\n",
    "X_train_weights = combined_df.loc[X_train.index, 'weight_preselection']\n",
    "X_test_weights = combined_df.loc[X_test.index, 'weight_preselection']\n",
    "\n",
    "# Impute and scale the features\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Convert data to torch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "X_train_weights_tensor = torch.tensor(X_train_weights.values, dtype=torch.float32)\n",
    "X_test_weights_tensor = torch.tensor(X_test_weights.values, dtype=torch.float32)\n",
    "\n",
    "# Create TensorDataset and DataLoader\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor, X_train_weights_tensor)\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor, X_test_weights_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available.\n",
      "Device name: Tesla T4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/sft.cern.ch/lcg/views/LCG_105_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available.\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU is not available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclassification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from itertools import cycle\n",
    "from scipy import interp\n",
    "\n",
    "# Define your DNN model for multiclass classification\n",
    "class SimpleDNN(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(SimpleDNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.fc5 = nn.Linear(64, 32)\n",
    "        self.fc6 = nn.Linear(32, 16)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.output = nn.Linear(16, num_classes)  # Output layer for num_classes\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)  # Apply Softmax for multiclass classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.relu(self.fc5(x))\n",
    "        x = self.relu(self.fc6(x))\n",
    "        x = self.softmax(self.output(x))  # Output probabilities\n",
    "        return x\n",
    "\n",
    "# Function to calculate accuracy for multiclass classification\n",
    "def calculate_accuracy(outputs, labels):\n",
    "    preds = torch.argmax(outputs, dim=1)  # Get the predicted class\n",
    "    correct = (preds == labels).float().sum()  # Compare with true labels\n",
    "    accuracy = correct / labels.size(0)  # Compute accuracy\n",
    "    return accuracy\n",
    "\n",
    "# Initialize the model\n",
    "input_dim = X_train.shape[1]  # Assuming X_train is your training data\n",
    "num_classes = 3  # Example number of classes\n",
    "model = SimpleDNN(input_dim, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()  # CrossEntropyLoss for multiclass classification\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# To store losses and accuracies\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "# Training loop\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_accuracy = 0.0\n",
    "    for X_batch, y_batch, weight_batch in train_loader:  # Assuming train_loader is your DataLoader\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Cast y_batch to LongTensor\n",
    "        y_batch = y_batch.long()\n",
    "\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        train_accuracy += calculate_accuracy(outputs, y_batch).item()\n",
    "\n",
    "    # Average training accuracy and loss\n",
    "    train_accuracy /= len(train_loader)\n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # Evaluate on test data\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_accuracy = 0.0\n",
    "    test_outputs = []\n",
    "    test_labels = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch, weight_batch in test_loader:  # Assuming test_loader is your DataLoader\n",
    "            # Cast y_batch to LongTensor for test data\n",
    "            y_batch = y_batch.long()\n",
    "\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            test_loss += loss.item()\n",
    "            test_outputs.append(outputs.numpy())\n",
    "            test_labels.append(y_batch.numpy())\n",
    "    \n",
    "    # Combine lists into numpy arrays for accuracy calculation\n",
    "    test_outputs = np.concatenate(test_outputs)\n",
    "    test_labels = np.concatenate(test_labels)\n",
    "    \n",
    "    # Calculate test accuracy\n",
    "    test_accuracy = calculate_accuracy(torch.tensor(test_outputs), torch.tensor(test_labels)).item()\n",
    "\n",
    "    # Append test losses and accuracies\n",
    "    test_losses.append(test_loss / len(test_loader))\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "    # Print epoch statistics\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "# Collect true labels and predictions for ROC curve calculation\n",
    "train_true = []\n",
    "train_preds = []\n",
    "test_true = []\n",
    "test_preds = []\n",
    "\n",
    "# During training, collect predictions and true labels\n",
    "for X_batch, y_batch, weight_batch in train_loader:\n",
    "    y_batch = y_batch.long()\n",
    "    outputs = model(X_batch)\n",
    "    train_preds.append(outputs.detach().numpy())\n",
    "    train_true.append(y_batch.numpy())\n",
    "\n",
    "# During testing, collect predictions and true labels\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch, weight_batch in test_loader:\n",
    "        y_batch = y_batch.long()\n",
    "        outputs = model(X_batch)\n",
    "        test_preds.append(outputs.detach().numpy())\n",
    "        test_true.append(y_batch.numpy())\n",
    "\n",
    "# Convert lists to numpy arrays for binarization and ROC calculation\n",
    "train_true = np.concatenate(train_true)\n",
    "train_preds = np.concatenate(train_preds)\n",
    "test_true = np.concatenate(test_true)\n",
    "test_preds = np.concatenate(test_preds)\n",
    "\n",
    "# Binarize the output for multiclass classification\n",
    "y_train_bin = label_binarize(train_true, classes=[i for i in range(num_classes)])\n",
    "y_test_bin = label_binarize(test_true, classes=[i for i in range(num_classes)])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve and ROC area for each class for training data\n",
    "fpr_train = dict()\n",
    "tpr_train = dict()\n",
    "roc_auc_train = dict()\n",
    "for i in range(num_classes):\n",
    "    fpr_train[i], tpr_train[i], _ = roc_curve(y_train_bin[:, i], train_preds[:, i])\n",
    "    roc_auc_train[i] = auc(fpr_train[i], tpr_train[i])\n",
    "\n",
    "# Compute ROC curve and ROC area for each class for test data\n",
    "fpr_test = dict()\n",
    "tpr_test = dict()\n",
    "roc_auc_test = dict()\n",
    "for i in range(num_classes):\n",
    "    fpr_test[i], tpr_test[i], _ = roc_curve(y_test_bin[:, i], test_preds[:, i])\n",
    "    roc_auc_test[i] = auc(fpr_test[i], tpr_test[i])\n",
    "    \n",
    "# Plot ROC curves\n",
    "def plot_roc_multiclass(fpr, tpr, roc_auc, title, num_classes):\n",
    "    colors = cycle(['blue', 'red', 'green', 'orange'])\n",
    "    plt.figure()\n",
    "    for i, color in zip(range(num_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                 label=f'Class {i} (AUC = {roc_auc[i]:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "\n",
    "# Plot the ROC curves for training data\n",
    "plot_roc_multiclass(fpr_train, tpr_train, roc_auc_train, 'ROC Curve (Train Data)', num_classes)\n",
    "\n",
    "# Plot the ROC curves for test data\n",
    "plot_roc_multiclass(fpr_test, tpr_test, roc_auc_test, 'ROC Curve (Test Data)', num_classes)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"/afs/cern.ch/user/s/sraj/sraj/www/CUA/HH-bbgg/plots_v1/NMSSM_signal/ML_plots/multiclass_ROC(after_variable_removal).png\")\n",
    "# plt.savefig(\"/afs/cern.ch/user/s/sraj/sraj/www/CUA/HH-bbgg/plots_v1/NMSSM_signal/ML_plots/multiclass_ROC(after_variable_removal).pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA  # Optional, for dimensionality reduction\n",
    "\n",
    "# Assuming you have already trained the model and you want to plot test data outputs\n",
    "\n",
    "# Get predictions on test data\n",
    "model.eval()\n",
    "test_preds = []\n",
    "test_labels = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch, weight_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        test_preds.append(torch.argmax(outputs, dim=1).numpy())  # Predicted classes\n",
    "        test_labels.append(y_batch.numpy())  # True classes\n",
    "\n",
    "# Combine batches into a single array\n",
    "test_preds = np.concatenate(test_preds)\n",
    "test_labels = np.concatenate(test_labels)\n",
    "\n",
    "# If the input has more than two dimensions, apply PCA to reduce to 2D for visualization\n",
    "if X_test_tensor.shape[1] > 2:\n",
    "    pca = PCA(n_components=2)\n",
    "    X_test_2d = pca.fit_transform(X_test_tensor.numpy())  # Reduce to 2D\n",
    "else:\n",
    "    X_test_2d = X_test_tensor.numpy()\n",
    "\n",
    "# Plot the predictions: signal (0) and background classes (1, 2)\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Signal class (0)\n",
    "plt.scatter(X_test_2d[test_preds == 0, 0], X_test_2d[test_preds == 0, 1], \n",
    "            color='blue', label='Signal', alpha=0.5)\n",
    "\n",
    "# Background class 1 (1)\n",
    "plt.scatter(X_test_2d[test_preds == 1, 0], X_test_2d[test_preds == 1, 1], \n",
    "            color='red', label='Background 1', alpha=0.5)\n",
    "\n",
    "# Background class 2 (2)\n",
    "plt.scatter(X_test_2d[test_preds == 2, 0], X_test_2d[test_preds == 2, 1], \n",
    "            color='green', label='Background 2', alpha=0.5)\n",
    "\n",
    "# Add plot labels and legend\n",
    "plt.title('Multiclass Classification: Signal and Backgrounds')\n",
    "plt.xlabel('Feature 1 (or PCA 1)')\n",
    "plt.ylabel('Feature 2 (or PCA 2)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Assuming the get_predictions function is already defined\n",
    "def get_predictions(loader, model):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, _ in loader:\n",
    "            outputs = model(inputs)\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    return np.array(all_preds), np.array(all_labels)\n",
    "\n",
    "# Get predictions for training and test data\n",
    "train_preds, train_true = get_predictions(train_loader, model)\n",
    "test_preds, test_true = get_predictions(test_loader, model)\n",
    "\n",
    "# Define bins\n",
    "bins = np.linspace(0, 1, 50)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Define distinct colors for each class\n",
    "colors = {\n",
    "    0: 'blue',\n",
    "    1: 'red',\n",
    "    2: 'green',\n",
    "    3: 'orange',\n",
    "    # Add more colors if you have more classes\n",
    "}\n",
    "\n",
    "# Plot histograms for training data for each class\n",
    "for i in range(num_classes):\n",
    "    color = colors.get(i, 'black')  # Fallback to black if class not found\n",
    "    class_mask_train = (train_true == i)\n",
    "    class_preds = train_preds[class_mask_train, i]  # Predictions for class i\n",
    "    \n",
    "    # Only plot if there are predictions for the class\n",
    "    if len(class_preds) > 0:\n",
    "        plt.hist(class_preds, bins=bins, color=color, alpha=0.5, \n",
    "                 label=f'Class {i} (Train)', density=True)\n",
    "\n",
    "# Calculate bin centers for the scatter plot\n",
    "bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "# Plot histograms for test data for each class\n",
    "for i in range(num_classes):\n",
    "    color = colors.get(i, 'black')  # Fallback to black if class not found\n",
    "    class_mask_test = (test_true == i)\n",
    "    class_preds_test = test_preds[class_mask_test, i]  # Predictions for class i\n",
    "    \n",
    "    # Only plot if there are predictions for the class\n",
    "    if len(class_preds_test) > 0:\n",
    "        hist_values, _ = np.histogram(class_preds_test, bins=bins, density=True)\n",
    "        plt.scatter(bin_centers, hist_values, color=color, alpha=0.7, \n",
    "                    label=f'Class {i} (Test)', marker='o', s=30, edgecolor='k')\n",
    "\n",
    "# Add background colors and legend\n",
    "plt.axvspan(0, 0.5, color='red', alpha=0.1)\n",
    "plt.axvspan(0.5, 1, color='blue', alpha=0.1)\n",
    "plt.axvline(0.5, color='k', linestyle='--')\n",
    "\n",
    "plt.xlabel('Classifier output')\n",
    "plt.ylabel('Normalized Yields')\n",
    "plt.xlim(0, 1)\n",
    "plt.legend(loc='upper center')\n",
    "plt.title('Classifier Output with PyTorch (Multiclass)')\n",
    "\n",
    "# Save and display the plot\n",
    "# plt.savefig(\"/afs/cern.ch/user/s/sraj/sraj/www/CUA/HH-bbgg/plots_v1/NMSSM_signal/ML_plots/multiclass_classifier_output(after_variable_removal).png\")\n",
    "# plt.savefig(\"/afs/cern.ch/user/s/sraj/sraj/www/CUA/HH-bbgg/plots_v1/NMSSM_signal/ML_plots/multiclass_classifier_output(after_variable_removal).pdf\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
